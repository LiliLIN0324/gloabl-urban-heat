{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# b_SOM_kmeans_process is designed to find the optimal number of clusters through SOM_kmeans and visualize the urban heat island (UHI) data for different cluster numbers.\n",
    "\n",
    "Here is the specific process breakdown:\n",
    "\n",
    "#### 04 Use SOM for feature extraction and K-means for clustering.\n",
    "> 04_00 The initial code that needs to be run every time.\n",
    "> 04_01 Use SOM to extract features, with the U-matrix as the distance, and apply K-means clustering to generate an SSE box plot for K within the K_range.\n",
    "> 04_02 Store both scaled and original data into CSV files.\n",
    "\n",
    "#### 05 Reorder clusters, obtain a sorted cluster CSV file from largest to smallest, and calculate the average value for the sorted clusters.\n",
    "\n",
    "#### 06 Analyze the final images of the moving average UHI data for different clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 04_00 每次运行的时候都需要先运行的代码。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "d = pd.read_csv(r'D:\\file\\c_moving_average(MA)\\223_cities_combined_moving_ave.csv')\n",
    "\n",
    "# 调整 pandas 显示选项\n",
    "pd.set_option('display.max_columns', None)  # 显示所有列\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "# 打印前 5 行数据\n",
    "print(d.head())\n",
    "\n",
    "# 获取并打印行数和列数\n",
    "print(f\"Rows: {d.shape[0]}, Columns: {d.shape[1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 文件路径\n",
    "bmus_path = r'D:\\file\\new som\\BMUs.npy'\n",
    "weights_path = r'D:\\file\\new som\\node_weights.npy'\n",
    "\n",
    "# 加载 .npy 文件\n",
    "bmus = np.load(bmus_path)\n",
    "node_weights = np.load(weights_path)\n",
    "\n",
    "# 打印基本信息\n",
    "print(\"BMUs shape:\", bmus.shape)\n",
    "print(\"BMUs data:\\n\", bmus)\n",
    "\n",
    "print(\"Node Weights shape:\", node_weights.shape)\n",
    "print(\"Node Weights data:\\n\", node_weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "output_file = r'D:\\file\\d_som\\node_weights.csv'\n",
    "\n",
    "# 创建一个空的 DataFrame 来保存结果\n",
    "df = pd.DataFrame(columns=['num_nodes'] + [f'feature_{i}' for i in range(node_weights.shape[1])])\n",
    "\n",
    "# 填充 DataFrame\n",
    "num_node = 0\n",
    "for i in range(node_weights.shape[0]):  # 假设有 1955 个节点\n",
    "    # 将每个节点的数据写入 DataFrame\n",
    "    df = df.append({'num_nodes': num_node, **{f'feature_{j}': node_weights[i, j] for j in range(node_weights.shape[1])}}, ignore_index=True)\n",
    "    num_node += 1\n",
    "\n",
    "# 将结果保存为 CSV 文件\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"数据已保存到 {output_file}\")\n",
    "##################################\n",
    "# 读取原始数据\n",
    "input_file = r'D:\\file\\d_som\\node_weights.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 假设有 17 行和 115 列的网格\n",
    "x_size, y_size = 17, 115\n",
    "\n",
    "# 创建 BMU_X 和 BMU_Y 列\n",
    "bmu_x = []\n",
    "bmu_y = []\n",
    "\n",
    "# 修正计算 BMU_X 和 BMU_Y\n",
    "for num_node in range(df.shape[0]):  # 假设有 1955 个节点\n",
    "    bmu_y.append(num_node % y_size)  # 使用模运算计算 Y 坐标\n",
    "    bmu_x.append(num_node // y_size)  # 使用整除计算 X 坐标\n",
    "\n",
    "# 将 BMU_X 和 BMU_Y 列添加到 DataFrame 中\n",
    "df['BMU_X'] = bmu_x\n",
    "df['BMU_Y'] = bmu_y\n",
    "\n",
    "# 保存新的 CSV 文件\n",
    "output_file = r'D:\\file\\d_som\\node_weights_with_bmu.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"数据已保存到 {output_file}\")\n",
    "################################\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 使用 K-means 聚类\n",
    "kmeans = KMeans(n_clusters=11, random_state=42)  # 设置你需要的聚类数\n",
    "kmeans.fit(node_weights)  # (num_nodes, feature_dim) = (1955, 2178)\n",
    "\n",
    "# 聚类结果：每个节点的标签\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# 读取原始的 CSV 文件\n",
    "input_file = r'D:\\file\\d_som\\node_weights_with_bmu.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 将 K-means 聚类的结果作为新的一列 'Cluster' 添加到 DataFrame\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# 保存修改后的 CSV 文件\n",
    "output_file = r'D:\\file\\d_som\\node_weights_with_bmu_and_cluster.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"数据已保存到 {output_file}\")\n",
    "################################\n",
    "\n",
    "# 读取 cluster 文件\n",
    "cluster_df = pd.read_csv(r'D:\\file\\d_som\\node_weights_with_bmu_and_cluster.csv')\n",
    "\n",
    "# 删除除了 'num_nodes', 'BMU_X', 'BMU_Y', 'Cluster' 以外的列\n",
    "cluster_df = cluster_df[['num_nodes', 'BMU_X', 'BMU_Y', 'Cluster']]\n",
    "\n",
    "# 查看处理后的 DataFrame\n",
    "print(cluster_df.head())\n",
    "\n",
    "# 如果需要，可以将处理后的数据保存为新的 CSV 文件\n",
    "output_file = r'D:\\file\\d_som\\node_weights_clean.csv'\n",
    "cluster_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"数据已保存到 {output_file}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 将 BMUs 数据转化为 DataFrame，并添加默认的索引列\n",
    "df = pd.DataFrame(bmus, columns=['BMU_X', 'BMU_Y'])\n",
    "\n",
    "# 添加默认的索引列并命名为 'index'\n",
    "df['index'] = df.index\n",
    "\n",
    "# 输出文件路径\n",
    "output_file = r'D:\\file\\d_som\\BMUs.csv'\n",
    "\n",
    "# 将 DataFrame 保存为 CSV 文件\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"数据已保存到 {output_file}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 cluster_file 和 bmus_file\n",
    "cluster_df = pd.read_csv(r'D:\\file\\d_som\\node_weights_with_bmu_and_cluster.csv')\n",
    "bmus_df = pd.read_csv(r'D:\\file\\d_som\\BMUs.csv')\n",
    "\n",
    "# 通过 BMU_X 和 BMU_Y 合并两个 DataFrame，使用左连接（left join）\n",
    "merged_df = pd.merge(bmus_df, cluster_df[['BMU_X', 'BMU_Y', 'Cluster']], on=['BMU_X', 'BMU_Y'], how='left')\n",
    "\n",
    "# 保存合并后的结果到新文件\n",
    "output_file = r'D:\\file\\d_som\\BMUs_with_cluster.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"数据已保存到 {output_file}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 06 分析不同的cluster的热岛滑动平均数最终的图像\n",
    "分析不同聚类的热岛滑动平均数，并检查数据中是否存在NaN或Inf值。\n",
    "清除异常值并生成最终的可视化图像。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取输入文件\n",
    "input_cities_file = pd.read_csv(r'D:\\file\\c_moving_average(MA)\\223_cities_combined_moving_ave.csv')\n",
    "input_bmus_cluster_file = pd.read_csv(r'D:\\file\\d_som\\BMUs_with_cluster.csv')\n",
    "\n",
    "# 将 BMUs 文件中的 Cluster 列根据默认索引赋值给城市数据文件\n",
    "input_cities_file['Cluster'] = input_bmus_cluster_file['Cluster']\n",
    "\n",
    "# 将结果保存为新的 CSV 文件\n",
    "output_cluster_file = r'D:\\file\\d_som\\223_cities_combined_moving_ave_cluster.csv'\n",
    "input_cities_file.to_csv(output_cluster_file, index=False)\n",
    "\n",
    "print(f\"数据已保存到 {output_cluster_file}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = pd.read_csv(output_cluster_file)\n",
    "print(file_name.iloc[0:5,0:3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sort_clusters_and_save(input_file_path, output_file_path):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(input_file_path, encoding='gbk')\n",
    "\n",
    "    # 将 \"X\" 和 \"Y\" 列作为索引，这两列不需要进行聚类\n",
    "    df.set_index([\"X\", \"Y\"], inplace=True)\n",
    "\n",
    "    # 排除不需要计算平均值的列\n",
    "    excluded_columns = [\"Place ID\", \"Sum ID\", \"City Name\", \"Place\"]\n",
    "    columns_to_average = [col for col in df.columns if col not in excluded_columns + [\"X\", \"Y\"]]\n",
    "\n",
    "    # 计算每个 Cluster 中的均值\n",
    "    cluster_means = df[columns_to_average].groupby(\"Cluster\").mean()\n",
    "\n",
    "    # 计算每个 Cluster 的均值，并按照均值排序\n",
    "    sorted_clusters = cluster_means.mean(axis=1).sort_values()\n",
    "\n",
    "    # **修正 cluster_mapping**\n",
    "    cluster_mapping = {old_cluster: new_cluster for new_cluster, old_cluster in enumerate(sorted_clusters.index)}\n",
    "\n",
    "    # 更新 Cluster 列的值为新的编号\n",
    "    df[\"Cluster\"] = df[\"Cluster\"].map(cluster_mapping)\n",
    "\n",
    "    # 保存更新后的 DataFrame\n",
    "    df.to_csv(output_file_path, encoding='gbk')\n",
    "\n",
    "K_range = [11]\n",
    "\n",
    "for K in K_range:\n",
    "    cluster_origin_file = r'D:\\file\\d_som\\223_cities_combined_moving_ave_cluster.csv'\n",
    "    cluster_processed_file = r'D:\\file\\d_som\\223_cities_combined_moving_ave_cluster_sort.csv'\n",
    "    sort_clusters_and_save(cluster_origin_file, cluster_processed_file)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = pd.read_csv(cluster_processed_file)\n",
    "print(file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = pd.read_csv(cluster_origin_file)\n",
    "print(file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pydlc import dense_lines\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_data(file_path, K, output_folder):\n",
    "    # 读取CSV文件\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # # 检查并清除包含 NaN 或 Inf 值的行\n",
    "    # if np.isnan(data.values).any() or np.isinf(data.values).any():\n",
    "    #     # 打印包含 NaN 和 Inf 值的行\n",
    "    #     print(\"原始数据框包含 NaN 或 Inf 值：\")\n",
    "    #     print(data)\n",
    "    #\n",
    "    #     # 打印 NaN 和 Inf 值的位置\n",
    "    #     nan_mask = np.isnan(data.values)\n",
    "    #     inf_mask = np.isinf(data.values)\n",
    "    #\n",
    "    #     nan_positions = np.where(nan_mask)\n",
    "    #     inf_positions = np.where(inf_mask)\n",
    "    #\n",
    "    #     if len(nan_positions[0]) > 0:\n",
    "    #         print(\"NaN 值及其位置：\")\n",
    "    #         for row, col in zip(nan_positions[0], nan_positions[1]):\n",
    "    #             print(f\"NaN 值在行 {row}, 列 {col}\")\n",
    "    #\n",
    "    #     if len(inf_positions[0]) > 0:\n",
    "    #         print(\"Inf 值及其位置：\")\n",
    "    #         for row, col in zip(inf_positions[0], inf_positions[1]):\n",
    "    #             print(f\"Inf 值在行 {row}, 列 {col}\")\n",
    "    #\n",
    "    #     # 删除包含 NaN 或 Inf 值的行\n",
    "    #     data = data.dropna().replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    #\n",
    "    #     # 打印清理后的数据框\n",
    "    #     print(\"清理后的数据框：\\ndata\")\n",
    "    # else:\n",
    "    #     print(\"数据框中不包含 NaN 或 Inf 值。\")\n",
    "\n",
    "    # 获取所有的 cluster 值\n",
    "    clusters = data['Cluster'].unique()\n",
    "\n",
    "    # 遍历每个 cluster\n",
    "    for cluster in clusters:\n",
    "        # 筛选出当前 cluster 的数据\n",
    "        cluster_data = data[data['Cluster'] == cluster]\n",
    "        print(cluster_data)\n",
    "        cluster_data_to_plot = cluster_data.iloc[:, 2:-5]\n",
    "\n",
    "        # 创建图表\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "        fig.patch.set_facecolor('white')  # 设置图形背景为白色\n",
    "        ax.set_facecolor('white')  # 设置轴背景为白色\n",
    "\n",
    "        # 获取列数\n",
    "        columns = np.arange(cluster_data_to_plot.shape[1])\n",
    "\n",
    "        # 绘制中值折线图\n",
    "        ax.plot(columns, cluster_data_to_plot.median(), linestyle='-', color='b', label='Median', linewidth=1, zorder=10)\n",
    "\n",
    "        # 使用 dense_lines 绘制密度线图\n",
    "        im = dense_lines(cluster_data_to_plot.values, x=columns, ax=ax, cmap='hot_r')\n",
    "        fig.colorbar(im, ax=ax)\n",
    "\n",
    "        # 设置图表布局和标题\n",
    "        ax.set_title(f'sorted_clustered_(K={K})_Cluster {cluster}', fontsize=20)\n",
    "        ax.set_xlabel('DAY', fontsize=14)\n",
    "        ax.set_ylabel('slide_TEM', fontsize=14)\n",
    "\n",
    "        # 删除竖直网格线\n",
    "        #ax.grid(which='major', axis='y', linestyle='dotted')\n",
    "        ax.grid(False)  # 完全去掉所有网格线\n",
    "\n",
    "        ax.set_ylim(-10, 10)  # 纵坐标是-100到100\n",
    "\n",
    "        # 设置 X 轴刻度和标签\n",
    "        ax.set_xticks(columns[::50])  # 每隔50个数据点设置一个刻度\n",
    "        ax.set_xticklabels(columns[::50])  # 设置对应刻度的标签为 columns 数组中的值\n",
    "\n",
    "        # 旋转 x 轴标签，避免重叠\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # 设置 x 轴范围，保证宽度匹配 figsize=(20, 6)\n",
    "        ax.set_xlim(0, len(columns))  # 设置 x 轴范围为 0 到总列数\n",
    "\n",
    "        # 调整布局，确保图像居中显示\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 保存图像为 JPG 格式\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder, rf'sorted_clustered_som_kmeans_data(K={K})_{cluster}.jpg')\n",
    "        plt.savefig(output_file, format='jpg', dpi=100, bbox_inches='tight')\n",
    "\n",
    "        # 显示图表\n",
    "        plt.show()\n",
    "\n",
    "K_range=[11]\n",
    "\n",
    "for K in K_range:\n",
    "    file_path = rf'D:\\file\\d_som\\223_cities_combined_moving_ave_cluster_sort.csv'\n",
    "    output_folder = rf'D:\\file\\d_som\\results\\picture\\K={K}'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    plot_cluster_data(file_path, K, output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = rf'D:\\file\\d_som\\223_cities_combined_moving_ave_cluster_sort.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install pydlc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 以城市为标准进行图像绘制"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pydlc import dense_lines\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_data(file_path, K, output_folder):\n",
    "    # 读取CSV文件\n",
    "    data = pd.read_csv(file_path)\n",
    "    # 获取所有的 cluster 值\n",
    "    cities = data['Place'].unique()\n",
    "\n",
    "    for city in cities:\n",
    "\n",
    "        city_data = data[data['Place'] == city]\n",
    "        place_id = city_data['City Name'].iloc[0]\n",
    "        # 获取所有的 cluster 值\n",
    "        clusters = city_data['Cluster'].unique()\n",
    "        # 遍历每个 cluster\n",
    "        for cluster in clusters:\n",
    "            # 筛选出当前 cluster 的数据\n",
    "            cluster_data = city_data[city_data['Cluster'] == cluster]\n",
    "            print(cluster_data)\n",
    "            cluster_data_to_plot = cluster_data.iloc[:, 2:-5]\n",
    "\n",
    "            # 创建图表\n",
    "            fig, ax = plt.subplots(figsize=(20, 6))\n",
    "            fig.patch.set_facecolor('white')  # 设置图形背景为白色\n",
    "            ax.set_facecolor('white')  # 设置轴背景为白色\n",
    "\n",
    "            # 获取列数\n",
    "            columns = np.arange(cluster_data_to_plot.shape[1])\n",
    "\n",
    "            # 计算统计量\n",
    "            median = cluster_data_to_plot.median()\n",
    "            q1 = cluster_data_to_plot.quantile(0)\n",
    "            q3 = cluster_data_to_plot.quantile(1)\n",
    "\n",
    "            # 绘制中位数线\n",
    "            ax.plot(columns, median, linestyle='-', color='b', label='Median', linewidth=1.5, zorder=10)\n",
    "\n",
    "            # 绘制 IQR 区间带\n",
    "            ax.fill_between(columns, q1, q3, color='orange', alpha=0.3, label='IQR (0% ~ 100%)')\n",
    "\n",
    "            # 添加图例\n",
    "            ax.legend()\n",
    "\n",
    "            # 设置图表布局和标题\n",
    "            ax.set_title(f'{city}_{place_id}_sorted_clustered_(K={K})_Cluster {cluster}', fontsize=20)\n",
    "            ax.set_xlabel('DAY', fontsize=14)\n",
    "            ax.set_ylabel('slide_TEM', fontsize=14)\n",
    "\n",
    "            # 删除竖直网格线\n",
    "            #ax.grid(which='major', axis='y', linestyle='dotted')\n",
    "            ax.grid(False)  # 完全去掉所有网格线\n",
    "\n",
    "            ax.set_ylim(-10, 10)  # 纵坐标是-100到100\n",
    "\n",
    "            # 设置 X 轴刻度和标签\n",
    "            ax.set_xticks(columns[::50])  # 每隔50个数据点设置一个刻度\n",
    "            ax.set_xticklabels(columns[::50])  # 设置对应刻度的标签为 columns 数组中的值\n",
    "\n",
    "            # 旋转 x 轴标签，避免重叠\n",
    "            plt.xticks(rotation=90)\n",
    "\n",
    "            # 设置 x 轴范围，保证宽度匹配 figsize=(20, 6)\n",
    "            ax.set_xlim(0, len(columns))  # 设置 x 轴范围为 0 到总列数\n",
    "\n",
    "            # 调整布局，确保图像居中显示\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # 保存图像为 JPG 格式\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            output_folder_2 = os.path.join(output_folder, f'{city}')\n",
    "            os.makedirs(output_folder_2, exist_ok= True)\n",
    "            output_file = os.path.join(output_folder_2 , rf'{city}_sorted_clustered_som_kmeans_data(K={K})_{cluster}.jpg')\n",
    "            plt.savefig(output_file, format='jpg', dpi=100, bbox_inches='tight')\n",
    "\n",
    "            # 显示图表\n",
    "            plt.show()\n",
    "\n",
    "K_range=[11]\n",
    "\n",
    "for K in K_range:\n",
    "    file_path = rf'D:\\file\\d_som\\223_cities_combined_moving_ave_cluster_sort.csv'\n",
    "    output_folder = rf'D:\\file\\d_som\\results\\picture\\K={K}'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    plot_cluster_data(file_path, K, output_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 以具体的线为标准进行图像绘制"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_data(file_path, find_id_path, K, output_folder):\n",
    "    # 1. 读取 CSV 和 Excel\n",
    "    data = pd.read_csv(file_path)\n",
    "    id_df = pd.read_excel(find_id_path, header=None)\n",
    "\n",
    "    # 2. 提取所有 Sum ID 和对应的 neighbor IDs（value_list）\n",
    "    all_ids       = id_df.iloc[:, 0].tolist()\n",
    "    all_neighbors = [row[1:].dropna().tolist() for _, row in id_df.iterrows()]\n",
    "\n",
    "    # 3. 主循环：对每个 Sum ID 画一张图\n",
    "    for sum_id, neighbor_ids in zip(all_ids, all_neighbors):\n",
    "        # 3.1 核心 DataFrame\n",
    "        core_df = data[data['Sum ID'] == sum_id]\n",
    "        if core_df.empty:\n",
    "            continue  # 如果 CSV 里没有这条，跳过\n",
    "\n",
    "        city     = core_df['Place'].iloc[0]\n",
    "        place_id = core_df['City Name'].iloc[0]\n",
    "        cluster  = core_df['Cluster'].iloc[0]\n",
    "\n",
    "        # 3.2 时间序列列的索引（第 3 列到倒数第 6 列）\n",
    "        ts_cols = core_df.columns[2:-5]\n",
    "        columns = np.arange(len(ts_cols))\n",
    "\n",
    "        # 3.3 核心曲线\n",
    "        core_series = core_df[ts_cols].iloc[0].tolist()\n",
    "\n",
    "        # 4. 绘图\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "        fig.patch.set_facecolor('white')\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # 4.1 先画所有 neighbor 曲线（浅灰色）\n",
    "        for nb_id in neighbor_ids:\n",
    "            nb_df = data[data['Sum ID'] == nb_id]\n",
    "            if nb_df.empty:\n",
    "                continue\n",
    "            nb_series = nb_df[ts_cols].iloc[0].tolist()\n",
    "            if len(nb_series) == len(columns):\n",
    "                ax.plot(columns, nb_series,\n",
    "                        color='lightgray',\n",
    "                        linewidth=1,\n",
    "                        alpha=0.5)\n",
    "        # 用一条“空”曲线做 legend proxy\n",
    "        ax.plot([], [], color='#999999', linewidth=1, alpha=0.5,\n",
    "                label='Neighbor Curves')\n",
    "\n",
    "        # 4.2 核心 city 曲线（深色粗线）\n",
    "        ax.plot(columns, core_series,\n",
    "                color='black',\n",
    "                linewidth=2,\n",
    "                label=f'Sum ID {sum_id}',\n",
    "                zorder=10)\n",
    "\n",
    "\n",
    "        # 5. 图例 & 美化\n",
    "        ax.set_title(f'{city}_{place_id}  K={K}  Cluster={cluster}', fontsize=18)\n",
    "        ax.set_xlabel('Day', fontsize=14)\n",
    "        ax.set_ylabel('slide_TEM', fontsize=14)\n",
    "        ax.legend(loc='upper right', fontsize=12)\n",
    "        ax.grid(False)\n",
    "        plt.xticks(columns[::50], rotation=90)\n",
    "        ax.set_xlim(0, len(columns))\n",
    "        ax.set_ylim(-12, 12)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 6. 保存 & 展示\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        out_path = os.path.join(\n",
    "            output_folder,\n",
    "            f'{city}_{place_id}_Sum ID{sum_id}_K{K}.jpg'\n",
    "        )\n",
    "        plt.savefig(out_path, dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == '__main__':\n",
    "    K = 11\n",
    "    file_path     = r'D:\\file\\d_som\\223_cities_combined_moving_ave_cluster_sort.csv'\n",
    "    find_id_path  = r'C:\\Users\\owner\\Desktop\\Sum ID.xlsx'\n",
    "    output_folder = r'D:\\file\\d_som\\results\\picture\\city_abnormal'\n",
    "    plot_cluster_data(file_path, find_id_path, K, output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 以cluster作为背景板"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 绘制map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_ids_map(file_path, find_id_path,\n",
    "                 x_col='X', y_col='Y',\n",
    "                 id_col='Sum ID',\n",
    "                 place_col='Place',\n",
    "                 place_name_col='City Name',\n",
    "                 output_folder='maps'):\n",
    "    \"\"\"\n",
    "    对 Excel 中所有 Sum ID，对应它们所在城市画地图：\n",
    "      • 城市里所有点：浅灰\n",
    "      • 对应 neighbor IDs：黑点\n",
    "    \"\"\"\n",
    "    # 1. 读取数据\n",
    "    data  = pd.read_csv(file_path)\n",
    "    id_df = pd.read_excel(find_id_path, header=None)\n",
    "\n",
    "    # 2. 提取所有 Sum ID 和对应的 neighbor IDs\n",
    "    all_neighbors = [row.dropna().astype(int).tolist()\n",
    "                     for _, row in id_df.iterrows()]\n",
    "\n",
    "    # 3. 确保输出目录\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 4. 对每个 Sum ID 单独画一张图\n",
    "    for neighbor_ids in all_neighbors:\n",
    "        sum_id = neighbor_ids[0]  # 当前 Sum ID（第一列）\n",
    "        core_df = data[data[id_col] == sum_id]\n",
    "        if core_df.empty:\n",
    "            continue\n",
    "\n",
    "        # 取出 place 和 place_name 的实际值\n",
    "        city           = core_df[place_col].iloc[0]\n",
    "        place_name_val = core_df[place_name_col].iloc[0]\n",
    "\n",
    "        # 整个城市所有点\n",
    "        city_df = data[data[place_col] == city]\n",
    "        xs = city_df[x_col]\n",
    "        ys = city_df[y_col]\n",
    "\n",
    "        # 开始绘图\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        fig.patch.set_facecolor('white')\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # 全部 city 点：浅灰\n",
    "        ax.scatter(xs, ys, s=40,\n",
    "                   color='lightgray', alpha=0.6,\n",
    "                   label=f'All points in {city}')\n",
    "\n",
    "        # 黑色点表示邻居节点\n",
    "        for nb in neighbor_ids:\n",
    "            nb_df = city_df[city_df[id_col] == nb]\n",
    "            if not nb_df.empty:\n",
    "                x = nb_df[x_col].values[0]\n",
    "                y = nb_df[y_col].values[0]\n",
    "                ax.scatter(x, y, s=60, color='black')\n",
    "                ax.annotate(str(nb), (x, y), textcoords=\"offset points\",\n",
    "                            xytext=(4, 4), ha='left', fontsize=9, color='black')\n",
    "\n",
    "\n",
    "        # 美化 & 保存\n",
    "        ax.set_title(f'{city} ({place_name_val}) — Map for Sum ID {sum_id}', fontsize=16)\n",
    "        ax.set_xlabel(f'{x_col} Coordinate', fontsize=12)\n",
    "        ax.set_ylabel(f'{y_col} Coordinate', fontsize=12)\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        filename = f'{city}_{place_name_val}_SumID{sum_id}_map.jpg'\n",
    "        out_path = os.path.join(output_folder, filename)\n",
    "        plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 用法示例\n",
    "if __name__ == '__main__':\n",
    "    plot_ids_map(\n",
    "        file_path=r'D:\\file\\d_som\\223_cities_combined_moving_ave_cluster_sort.csv',\n",
    "        find_id_path=r'C:\\Users\\owner\\Desktop\\Sum ID.xlsx',\n",
    "        x_col='X',\n",
    "        y_col='Y',\n",
    "        id_col='Sum ID',\n",
    "        place_col='Place',\n",
    "        place_name_col='City Name',\n",
    "        output_folder=r'D:\\file\\d_som\\results\\picture\\city_abnormal\\maps'\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python11",
   "language": "python",
   "display_name": "Python 3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
